{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akislenkova/ML-Data/blob/main/Titanic_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"File successfully downloaded as {filename}\")\n",
        "    else:\n",
        "        print(\"Failed to retrieve the file. Status code:\", response.status_code)"
      ],
      "metadata": {
        "id": "nb_roroBSNYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/CreekCS/Lab_3_Titanic/refs/heads/main/Titanic_data.csv\"\n",
        "filename = \"Titanic_data.csv\"\n",
        "download_file(url, filename)"
      ],
      "metadata": {
        "id": "-aS2SUl1SPPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "################################################################################################################\n",
        "Lab #3 - The Titanic: Probability of Surviving\n",
        "The point of this lab is to show how the Lab #2 techniques can be used on a more complex linear model.  We will\n",
        "insert some activation functions into the model to make it compute a probability.  (Therefore this model will\n",
        "output a probability, as opposed to Lab #2 which output a real value - Brodie's weight)  This lab...\n",
        "    -uses Tensorflow and Keras to implement a model with 2 hidden layers, having 7 input Features and 120\n",
        "     Weights  (i.e. a 121-D model of cost) .\n",
        "    -introduces the idea of splitting the data into Training and Testing datasets,\n",
        "    -shows how to manipulate values within the input Python arrays so as to build appropriate Training and Test\n",
        "     datasets,\n",
        "    -introduces data scaling using the StandardScaler to scale the individual features of the Training dataset,\n",
        "    -employs the non-linear RELU and Sigmoid activation functions, and\n",
        "    -introduces a Confusion Matrix to analyze the results.\n",
        "\n",
        "Needed files:  Titanic_data.csv\n",
        "# Author: R. Bourquard - Dec 2020\n",
        "################################################################################################################\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "ozS52q3mU6dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #1:  The input file\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# In the morning hours of April 15, 1912, the cruise ship, Titanic, sank with great loss of life.  Ironically, it\n",
        "# was the \"unsinkable\" ship's maiden voyage.  There were approximately 2,200 passengers and crew aboard.  At around\n",
        "# midnight, the ship struck an iceberg.   It sank 2 hours later, resulting in the deaths of approximately 1,500\n",
        "# people.\n",
        "#\n",
        "# Our objective is to build a model (algorithm and weights) that will allow us to input a passenger's data (the\n",
        "# Features) and be able to predict whether that passenger survived or perished.  In other words, we need to be able\n",
        "# to find and appropriately weight those Features which were important to survival.  Were your survival odds improved\n",
        "# if you were in 1st class?  If you were female?  If you were young?  If you were alone?\n",
        "#\n",
        "# The input csv file, 'Titanic_data.csv', contains information about all the passengers.  (The file's dataset was\n",
        "# built by Khashayar Baghizadeh Hosseini, and is available at https://www.kaggle.com/heptapod/titanic.)\n",
        "# There were approximately 1,310 passengers on the ship, and this file includes as many rows; one row for each\n",
        "# passenger.\n",
        "#\n",
        "# The file contains 9 columns of data for each passenger.  Each column is a Feature.  Each row is a Training Example.\n",
        "# Columns in csv files are numbered starting with \"1\".  The numbers in brackets [] are column numbers as stored in a\n",
        "# python array (starting with \"0\").\n",
        "# Column 1 [0]  - An arbitrary Passenger ID (1:1310).\n",
        "# Column 2 [1] -  The age in years of the passenger.\n",
        "# Column 3 [2] -  The fare paid in dollars.  (These fares seem incredibly cheap because of the rampant inflation\n",
        "#                 that has occurred since 1912.)\n",
        "# Column 4 [3] -  The passenger's sex:  0=male, 1=female\n",
        "# Column 5 [4] -  SibSp:  The total number of siblings plus spouse who were accompanying the passenger.\n",
        "# Column 6 [5] -  ParCh:  The total number of parents or children who were accompanying the passenger.\n",
        "# Column 7 [6] -  Passenger Class:  1=1st Class, 2=2nd Class, 3=3rd Class\n",
        "# Column 8 [7] -  Embarkation point:  0=Cherbourg, 1=Queenstown, 2=Southhampton\n",
        "# Column 9 [8] -  Survived:  1=survived, 0=perished (This is the Ground Truth value)\n",
        "#\n",
        "# Column 1, the passenger ID, is of no value in this problem.  And Column 9, the survived flag, is the answer (or\n",
        "# Ground Truth).  So that leaves 7 columns of interest to be used as our input Features.\n",
        "# We could guess which of these features might be most important to survival, but it would be difficult/impossible\n",
        "# to guess a useful relative weight to all 7.  A deep neural network can do this for us.  Once it derives the\n",
        "# weights we can input a new passenger's 7 Features and use them to calculate a prediction of the probability of\n",
        "# survival.\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# READ THE INPUT DATA\n",
        "input_filename = filename\n",
        "input_data = np.loadtxt(input_filename, dtype='float32', delimiter=\",\", skiprows=1)\n",
        "print('input_data:', input_data.shape)\n",
        "# PRINT SOME OF THE INPUT DATA EXAMPLES\n",
        "print('    ID     Age    Fare    Sex     SibSp     ParCh     Class   Embark   GTSurvive')\n",
        "for i in range(0, 19):\n",
        "    print('{:6.0f}'.format(input_data[i,0]), '  ',\n",
        "          '{:4.0f}'.format(input_data[i,1]), '  ',\n",
        "          '${:3.0f}'.format(input_data[i,2]), ' ',\n",
        "          '{:3.0f}'.format(input_data[i,3]), '    ',\n",
        "          '{:3.0f}'.format(input_data[i, 4]), '     ',\n",
        "          '{:3.0f}'.format(input_data[i, 5]), '     ',\n",
        "          '{:3.0f}'.format(input_data[i, 6]), '    ',\n",
        "          '{:3.0f}'.format(input_data[i, 7]), '     ',\n",
        "          '{}'.format(input_data[i, 8]==1) )\n"
      ],
      "metadata": {
        "id": "7ZstrxLPVPDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #2:  Split the input data into a Training dataset and a Testing dataset\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# This model (algorithm and weights) is far more complex than the previous Brodie Weight model.  Once we have\n",
        "# determined the weights, there will be no way we can plot an imaginary boundary in 7-D Feature-space between those\n",
        "# passengers who survived and those who perished to visually assess how the well model works.  Instead, we will\n",
        "# assess how well the model works by simply using part of initial data we were given.\n",
        "#\n",
        "# To accomplish this, we will divide the data into two groups: a \"Training Examples\" dataset, and a \"Testing\n",
        "# Examples\" dataset.  The Training Examples dataset will be used to train our model's weights by having it try to\n",
        "# predict the survival flags (Ground Truth Values) contained therein.  After running many Epochs, the \"final\" model\n",
        "# should be able to predict the actual Ground Truth outcomes fairly well.  We will eventually run statistics on\n",
        "# the \"final\" model to measure whether it was a good choice.\n",
        "#\n",
        "# Once we have a \"final\" model, the Testing Examples dataset will be input into the \"final\" model and it will\n",
        "# predict whether the Testing Examples' passengers survived.  We already know whether these passengers survived\n",
        "# since we have their Ground Truth values, so if our model is really good, its predictions will match what actually\n",
        "# happened.  Since our model has never \"seen\" these Testing Examples before, it's statistically fair to compare\n",
        "# the predicted outcomes to the actual Ground Truth outcomes to evaluate how good our model will be on new data.\n",
        "#\n",
        "# This code splits the input data into a Training Examples dataset and a Testing Examples dataset.  80% of the rows\n",
        "# (training_split=0.80) will be randomly selected and put in the Training Examples dataset, the remainder will be put\n",
        "# in the Testing Examples dataset.  Specifying the 'seed' causes the random generator to split the data identically\n",
        "# each time this program is run, so comparisons can be made between runs.\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# SPLIT THE INPUT DATA INTO A TRAINING DATASET AND A TESTING DATASET\n",
        "training_split = 0.80\n",
        "seed = 42\n",
        "train_data, test_data = train_test_split(input_data, train_size=training_split, random_state=seed)\n",
        "print('train_data:', train_data.shape)\n",
        "print('test_data:', test_data.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nAT1ErDoVlQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #3:  Split-off the Features and the Ground Truth from the Training and Testing datafiles\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# There are 9 values, in csv columns 1:9 [python columns 0:8].  However, column 1 [0] is the Passenger ID, which\n",
        "# is (by my intuition) of no value to deciding who survived, so it is skipped.  Column 9 [8] is the 'survived'\n",
        "# flag, which is our Ground Truth.  So our input Features will be columns 2:8 [1:7].  This code simply splits the\n",
        "# Examples into a 2-D Feature matrix (rows=Examples, columns=the 7 Features), and the Ground Truth into a matching\n",
        "# 1-D array (aka a vector).\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# SEPARATE-OUT THE FEATURES AND THE GROUND TRUTH\n",
        "nFeatures = 7\n",
        "ground_truth_col = 8\n",
        "# for the training data\n",
        "train_X = train_data[:,1:nFeatures+1]   # The 7 features are in csv columns 2-8 [1:7]\n",
        "train_truth = train_data[:,ground_truth_col]   # The 'survived' flag is in csv column 9 [8]\n",
        "print('train_truth shape[0]', train_truth.shape[0])\n",
        "# for the test data\n",
        "test_X = test_data[:,1:nFeatures+1]   # The 7 features are in csv columns 2-8 [1:7]\n",
        "test_truth = test_data[:,ground_truth_col]   # The 'survived' flag is in column 9 [8]\n",
        "print('train_X', train_X.shape)\n",
        "print('train_truth', train_truth.shape)\n",
        "print('test_X', test_X.shape)\n",
        "print('test_truth', test_truth.shape)\n",
        "print()\n",
        "n_test_examples = test_X.shape[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "G9rEOBI5VrUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #4:  Scale the values in the datasets\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# As with the Brodie Weights model, there is a great difference in magnitude between the values of the various\n",
        "# Features.  A typical way to solve this is to individually scale the input Features, so they are all\n",
        "# normalized (centered on zero with a standard deviation of 1).  (This means that after scaling they will mostly\n",
        "# be between -1 and +1.)  The following code does this.  Each Feature column is scaled individually across\n",
        "# all the Training Example rows.\n",
        "#\n",
        "# Note that we compute a scaling object (scaler_obj), which contains the derived scale factors.  It is calculated\n",
        "# from just the Training Features.  Once we have it, it will be applied to both the Training Features and the Test\n",
        "# Features, since they both must be scaled identically.\n",
        "#\n",
        "# (Typically, after this is done, the scaling object is output and saved in a file for later use, since it must be\n",
        "# applied to the Features of every subsequent input to the model.  In this program, we don't save the scaling\n",
        "# object because there are no additional Examples to be run in some later program.)\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# SCALE EACH FEATURE TO BE CENTERED ON ZERO, WITH A STANDARD DEVIATION OF 1\n",
        "scaler_obj = preprocessing.StandardScaler().fit(train_X)   # scaler_obj will scale each Feature (column) independently\n",
        "train_X_scaled = scaler_obj.transform(train_X)  # scale each Training Feature (column)\n",
        "test_X_scaled = scaler_obj.transform(test_X)  # scale each Test Feature (column)\n",
        "\n",
        "\n",
        "print('train_X', train_X_scaled.shape)\n",
        "print('train_truth', train_truth.shape)\n",
        "print('test_X', test_X_scaled.shape)\n",
        "print('test_truth', test_truth.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "RsVc3qRJVyZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #5:  The DNN Model\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# Now we are ready to build our model.  We use Keras to define the layers:\n",
        "#  - There is one Input Layer, which simply receives the 7 Features.\n",
        "#  - There are 2 Hidden (Activation) Layers; both of which use ReLU activation functions.\n",
        "#  - And 1 Output Layer, which uses a single Sigmoid activation function, since we want the model to predict just\n",
        "#    one outcome = the probability of survival.\n",
        "# You can lookup ReLU and Sigmoid activation functions on your own, to see how they modify their input data.\n",
        "# The specified optimizer is 'adam' which is basically a gradient descent modified by a momentum factor.\n",
        "# The optimizer will minimize the loss, which will be calculated as 'binary_crossentropy' because we want only\n",
        "# a single probability value representing the 2 (binary) outcomes:  survived or perished.\n",
        "# The screen print shows the model has 120 weights and biases (\"Trainable params\") to be trained.\n",
        "#\n",
        "#\n",
        "# AI MODELING NOTE:  Why are there 120 weights?\n",
        "# -- FOR THE INPUT LAYER:\n",
        "#    There are 7 input features composing the Input Layer.\n",
        "# -- FOR HIDDEN LAYER 1:\n",
        "#    Because we are using 'Dense' connections, each of the 7 inputs connects to every neuron in Hidden Layer 1.\n",
        "#    There are 7 neurons in Hidden Layer 1. This means that each of the 7 input features will connect to all\n",
        "#    7 Hidden Layer 1 neurons. Since each connection has an independent weight, there will be 7*7 = 49 weights.\n",
        "#    In addition, each neuron in Hidden Layer 1 has a bias value, so there are also 7 bias values. So the total\n",
        "#    for Hidden Layer 1 will be 49 + 7 = 56 weights and biases.\n",
        "# -- FOR HIDDEN LAYER 2:\n",
        "#    Because we are using 'Dense' connections, each of the 7 Hidden Layer 1 neurons connects to every neuron in\n",
        "#    Hidden Layer 2. Since there are 7 neurons in Hidden Layer 2, there will be 7*7 = 49 weights. In addition,\n",
        "#    each neuron in Hidden Layer 2 also has a bias value, so there are 7 bias values. So the total for Hidden\n",
        "#    Layer 2 will be 49 + 7 = 56 weights and biases.\n",
        "# -- FOR THE OUTPUT LAYER:\n",
        "#    Because we are using 'Dense' connections, each of the 7 Hidden Layer 2 neurons connects to the single Output\n",
        "#    Layer neuron. This means that there will be 7 more weights. In addition, the output neuron has a bias value,\n",
        "#    so the total for the Output Layer will be 7 + 1 = 8 weights and biases.\n",
        "# Therefore the total number of weights and biases is 56 + 56 + 8 = 120 weights and biases. The weights and biases\n",
        "# are called \"Trainable params\" in the output below.\n",
        "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "\n",
        "# BUILD THE TENSORFLOW MODEL\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(shape=[nFeatures,]))        # Input Layer\n",
        "model.add(keras.layers.Dense(nFeatures, activation='relu'))   # Hidden Layer 1\n",
        "model.add(keras.layers.Dense(nFeatures, activation='relu'))   # Hidden Layer 2\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))        # Output Layer\n",
        "model.summary()\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5tOn6sS5V472"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #6:  Run the model\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# The TensorFlow 'fit' method is used to compute the best weights for the Training Examples.  The 120 weights\n",
        "# are trained over 10 epochs.  Note that the sigmoid activation function outputs a probability of survival (between\n",
        "# 0 and 1).  We will consider anything >= 0.5 as a prediction of survival.  The comparison of the Training Examples\n",
        "# to their Ground Truth values are stored in the 'history' object for plotting.\n",
        "#\n",
        "# The Test Examples are also input to the fit method and the predictions from our evolving model (derived from the\n",
        "# Training Examples) are compared against the matching Test Ground Truth values.  This gives a validation\n",
        "# measure of how the model will do when data it has never seen before are input.  The comparison of the Test\n",
        "# Examples to their Ground Truth values are also stored in the 'history' object for plotting.\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# FIND BEST VALUES FOR THE 120 WEIGHTS\n",
        "nEpochs = 10\n",
        "history = model.fit(train_X_scaled, train_truth, batch_size=1200, epochs=nEpochs, validation_data=(test_X_scaled, test_truth))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bDhr5FQvWAzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #7:  Print/plot information about its accuracy\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# To get a measure of the accuracy of the new model, the 'evaluate' method can be run on both the Training\n",
        "# Examples and their matching Ground Truth values, and on the Test Examples and their matching Ground Truth\n",
        "# values.  For each set of examples, the 'loss' shows the Cost of the errors (i.e. false positives and\n",
        "# negatives).  Lower Cost is better.  The 'accuracy' shows how well the predicted probabilities matched the\n",
        "# Ground Truth values.  Higher accuracy is better.\n",
        "#\n",
        "# For this model's Training data, after 10 epochs the cost was 0.44 (arbitrary units), and the prediction\n",
        "# accuracy was about 79%.  This means given a Training Example passenger's Features, we can predict that\n",
        "# passenger's survival with about 79% accuracy.  This give us a measure of whether our model is appropriate for\n",
        "# the task at hand.\n",
        "# For this model's Test Data (USING THE WEIGHTS DERIVED FROM THE TRAINING EXAMPLES) the cost was 0.49, and\n",
        "# the prediction accuracy was about 77%.  This means given any passenger's Features, we can predict that\n",
        "# passenger's survival with about a 77% accuracy.  This gives us a measure of whether our model will work well\n",
        "# on entirely new data.\n",
        "#\n",
        "# We would expect the Training Examples' accuracy and Cost to be better than the Test Examples' accuracy and\n",
        "# Cost, since the model weights were derived from the Training Examples!  Regardless, the accuracy and Cost\n",
        "# for the Test Examples should be similar to those of the Training Examples, if the model we derived is any good.\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# PRINT AND PLOT STATISTICS\n",
        "score,accuracy = model.evaluate(train_X_scaled, train_truth, batch_size=16, verbose=0)\n",
        "print(\"Train score (cost)       = {:.2f}\".format(score))\n",
        "print(\"Train accuracy (accuracy)= {:.2f}\".format(accuracy))\n",
        "score,accuracy = model.evaluate(test_X_scaled, test_truth, batch_size=16, verbose=0)\n",
        "print(\"Test score (val_cost)    = {:.2f}\".format(score))\n",
        "print(\"Test accuracy (val_accuracy)= {:.2f}\".format(accuracy))\n",
        "\n",
        "\n",
        "# PLOT COST AND ACCURACY\n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1)\n",
        "plt.suptitle(\"Cost and Accuracy for \" + str(nEpochs) + \" epochs\")\n",
        "plt.title(\"'loss' = Cost      'val' = test scores\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3vQTDsmAWIoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #8:  Print a Confusion Matrix on the Test Examples\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# One way to see the relationship between cost and accuracy is to compute a Confusion Matrix.  It shows all the\n",
        "# Testing Examples' outcomes collected into a square of 4 groupings for quick comparison:\n",
        "#\n",
        "#                        true negatives,    false negatives\n",
        "#                        false positives,   true positives\n",
        "#\n",
        "# The Confusion Matrix is run on the Test Examples because those are the best indicators of how accurate the\n",
        "# model is.\n",
        "#\n",
        "# Ideally, the \"true\" diagonal of 'true negatives' and 'true positives' (in this case, correct 'perished'\n",
        "# and correct 'survived' predictions) should be far greater than the \"false\" diagonal of 'false positives' and\n",
        "# 'false negatives' (in this case, incorrect 'survived' and incorrect 'perished' predictions).\n",
        "#\n",
        "# For this Titanic lab, the biggest concern is probably the number of \"false positives\", because these are\n",
        "# people the DNN predicted to survive, who actually perished!  The consequence of being wrong was fatal!  For a\n",
        "# useful model, you would want to minimize these outcomes.  For other situations, however, the number of \"false\n",
        "# negatives\" might be more important.  For example, if the DNN was suggesting surgery, a \"false negative\"\n",
        "# would mean a needed surgery was NOT performed.  You might wish to minimize this category instead.\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# PRINT A CONFUSION MATRIX FOR THE TEST EXAMPLES\n",
        "probabilities = model.predict(test_X_scaled)   # the probability that the passengers survived\n",
        "# convert the probabilities to predictions (0 or 1) for comparison with the ground truth\n",
        "min_for_true = 0.5   # if the probability is >= 0.5, then assume the passenger survived\n",
        "vector_int = np.vectorize(int)\n",
        "predictions = vector_int(probabilities + min_for_true)    # The int truncates any fractional part of the sum\n",
        "((n_true_negatives, n_false_positives), (n_false_negatives, n_true_positives)) \\\n",
        "      = confusion_matrix(test_truth, predictions)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print('Confusion Matrix for Predicted Survival.  Passenger counts for', n_test_examples, 'Test Examples')\n",
        "print()\n",
        "print('     Correctly Predicted will Perish:          ', n_true_negatives, ' | ',\n",
        "      n_false_negatives, ' :Predicted Perished, but actually Survived')\n",
        "print('                              ------------------------------------------------')\n",
        "print('     Predicted Survive, but actually Perished:  ', n_false_positives, ' | ',\n",
        "      n_true_positives, ' :Correctly Predicted will Survive')\n",
        "print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1p-FC-pCWQIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iaRAk9aRvxi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# DISCUSSION #9:  Print some examples\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "# Now that we have derived a model and weights, we can use it to predict the outcomes of new passengers.  However,\n",
        "# there are no new passengers, so here I've simply predicted the outcomes of the first 20 Test Examples.  The table\n",
        "# shows the passenger data, the resulting prediction, and the actual Ground Truth outcome for comparison.\n",
        "# The predictions should be correct about 77% of the time.\n",
        "#\n",
        "# Note that the model and its weights are relatively opaque.  It can predict who will survive, but it's difficult\n",
        "# to know why!  This is a simple model with only 120 weights.  Yet it would be nearly impossible to translate those\n",
        "# weights into an understanding of the relative importance of each input Feature.  Therefore, the model is rather\n",
        "# like a black box.  Sadly, this opacity is typical of AI models.\n",
        "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "# PRINT SOME OF THE INPUT DATA EXAMPLES\n",
        "print()\n",
        "print()\n",
        "print('Some Passenger Data, with Predictions and Actual Outcomes...')\n",
        "print(' Age    Fare    Sex     SibSp     ParCh     Class   Embark   [Pred]Survived    [Actual]Survived')\n",
        "for i in range(0, 20):\n",
        "    print('{:4.0f}'.format(test_X[i,0]), '  ',\n",
        "          '${:3.0f}'.format(test_X[i,1]), ' ',\n",
        "          '{:3.0f}'.format(test_X[i,2]), '    ',\n",
        "          '{:3.0f}'.format(test_X[i, 3]), '     ',\n",
        "          '{:3.0f}'.format(test_X[i, 4]), '     ',\n",
        "          '{:3.0f}'.format(test_X[i, 5]), '    ',\n",
        "          '{:3.0f}'.format(test_X[i, 6]), '      ',\n",
        "          '{}'.format(predictions[i]==1), '         ',\n",
        "          '{}'.format(test_truth[i]==1)\n",
        "          )\n"
      ]
    }
  ]
}